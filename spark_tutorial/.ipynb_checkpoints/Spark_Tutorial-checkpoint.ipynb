{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the 3 RDDs from the different datasets from Amazon product reviews. Note that it does not move the data at this stage due to the lazy evaluation nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start by understanding the core concepts of Apache Spark. Themost essential compenent in Spark is the SparkContext. In pySpark, SparkContext is instaciated by the variable `sc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sc.parallelize()` function to convert a simple python list to a RDD. Let's first create a dataset in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "\n",
    "partition_data = []\n",
    "\n",
    "for i in range(0,num):\n",
    "    for j in range(0,i):\n",
    "        partition_data.append((i,j))\n",
    "\n",
    "print partition_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use `sc.parallelize()` function to convert this list to a Spark RDD. Then lets use `rdd.getNumPartitions()` function to figure out how many partitions the RDD has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_rdd = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function allows us to visualise how the data is structured inside the RDD in multiple partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_human_readable(rdd_obj):\n",
    "    \"\"\"Takes in an RDD and prints the contents of each partition in a single line\n",
    "    \n",
    "    Args: \n",
    "        rdd_obj (RDD): spark RDD with data\n",
    "    \"\"\"\n",
    "    partition_view = rdd_obj.mapPartitions(lambda l: [l]).map(list).collect()\n",
    "    \n",
    "    print \"Number of Partitions in the RDD : {}\".format(len(partition_view))\n",
    "    print \"\\n\\n The partition contents are as follows: \\n\"\n",
    "\n",
    "    for partition in partition_view:\n",
    "        print partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the `partiion_rdd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_human_readable(partition_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations with no shuffles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transformations with no shuffles are carried out. The partition structure doesn't change. Data remains in the same partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply a `map()` function on the `(k, v)` key-value pair to replace v with `v+1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = ??? # (v+1)\n",
    "to_human_readable(mapped_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply a `filter()` function to `(k, v)` key-value pairs such that only data points where v values devisible by 2 are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rdd = ??? # v%2 == 0\n",
    "to_human_readable(filtered_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to look at a real world dataset from Amazon about its products and product reviews to use Apache spark to process this data and extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and managing RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using the Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fashion = sc.textFile('Data/Reviews/fashion.json')\n",
    "electronics = sc.textFile('Data/Reviews/electronics.json')\n",
    "sports = sc.textFile('Data/Reviews/sports.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has happened, why is that?\n",
    "In Spark, some operations are *transformations*, which are lazily evaluated and others are *actions*.\n",
    "\n",
    "Read more here: http://spark.apache.org/docs/latest/programming-guide.html#transformations\n",
    "\n",
    "Now lets look at the first line of one of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's clear that the data is loading properly, we can do some basic data exploration. Lets count the number of items in each dataset using `rdd.count()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print \"fashion has {0} rows, electronics {1} rows and sports {2} rows\\n\".format(_, _, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all three of these datasets are product reviews, we can treat them as a single dataset by using `union` function to get the union of the RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union_of_rdds = fashion.union(electronics)\n",
    "print union_of_rdds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of loading files is by using a list of comma-separated file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_records = sc.textFile('Data/Reviews/fashion.json,Data/Reviews/electronics.json,Data/Reviews/sports.json')\n",
    "text_records.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use POSIX filepath wildcards to input data using `sc.TextFile()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_records = sc.textFile(???)\n",
    "text_records.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first line, we can see that the data is in json format. We can now parse the file using the json library. In order to parally parse each line from `data` rdd, we can use a linewise transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = ??? # json.loads(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine we want to know the number of lines in each partition. For that, we need to access the data in each single partition and run operations on them instead of on each row.\n",
    "\n",
    "For this, we will use mapPartitionsWithIndex which takes a partition index and an iterator over the data as arguments. Each function in the API is documented in: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = data.mapPartitionsWithIndex(lambda splitIndex, it: [(splitIndex, len([x for x in it]))])\n",
    "\n",
    "for num_partition, count_partition in indexed_data.collect():\n",
    "    print \"partition {0} has {1} rows\".format(num_partition, count_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we have been tasked to do is **to get the minimum and maximum number of reviews per product**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASIN_FIELD = 'asin'\n",
    "\n",
    "# The rdd product_num will contain (product_asin, total_number_reviews)\n",
    "product_num = ???\n",
    "\n",
    "\n",
    "# What are the maximum and minimum number of reviews?\n",
    "max_num = ???\n",
    "min_num = ???\n",
    "\n",
    "print \"Max number of reviews is {0}, min number of reviews is {1}\".format(max_num, min_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/reducebykey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the above `map()` transformations using `values()`, `min(key=)` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the maximum and minimum number of reviews?\n",
    "max_num = product_num.values().max()\n",
    "\n",
    "# if you want to retain to the key, use mapValues\n",
    "min_asin, min_num = product_num.min(key=lambda x: x[1])\n",
    "\n",
    "print \"Max number of reviews is {0}, min number of reviews is {1}\".format(max_num, min_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Find the maximum score that has been given to a review of each product in the product reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining multiple sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining data based on a data column is very useful in data munging. In this section we try to join the product reviews dataset with the product metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to join the product reviews by users to the product metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_metadata = (sc.textFile('Data/Products/sample_metadata.json').\n",
    "                       map(lambda x: json.loads(x)))\n",
    "\n",
    "print product_metadata.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the product category is a multilevel list. We introduce `flatten_categories()` function to flatten these multilevel categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORIES_FIELD = 'categories'\n",
    "\n",
    "def flatten_categories(line):\n",
    "    \"\"\"Takes the product category record and modifies it by converting the multilevel product category\n",
    "    field to flattened version\n",
    "    \n",
    "    Args: \n",
    "        line (dict): product record\n",
    "    Returns:\n",
    "        line (dict): modified product record\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    old_cats = line[CATEGORIES_FIELD]\n",
    "    line[CATEGORIES_FIELD] = [item for sublist in old_cats for item in sublist]\n",
    "    return line\n",
    "\n",
    "product_metadata = product_metadata.map(flatten_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to join the review data to the metadata about the product. We can use the ASIN for that, which is a unique identifier for each product. In order to do a join, we need to turn each structure into key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_val_data = data.map(lambda x: (x[ASIN_FIELD], x))\n",
    "\n",
    "key_val_metadata = product_metadata.map(lambda x: (x[ASIN_FIELD], x))\n",
    "\n",
    "print \"We are joining {0} product reviews to {1} rows of metadata information about the products.\\n\".format(key_val_data.count(),\n",
    "                                                                                                            key_val_metadata.count())\n",
    "print \"First row of key_val_data:\"\n",
    "print key_val_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"number partitions key_val_data: \", \n",
    "print key_val_data.getNumPartitions()\n",
    "print \"number partitions key_val_metadata: \", \n",
    "print key_val_metadata.getNumPartitions()\n",
    "print\n",
    "\n",
    "joined = ???\n",
    "\n",
    "key, (review, product) = joined.first()\n",
    "print \"For key {0}:\\n\\nthe review is {1}\\n\\nthe product metadata is {2}.\\n\".format(key, review, product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of output partitions of the join? To understand this, the best is to refer back to the Pyspark source code: https://github.com/apache/spark/blob/branch-1.3/python/pyspark/join.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: what is the number of partitions of the joined dataset?\n",
    "\n",
    "print \"There are {0} partitions\".format(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to manipulate, we will change the structure of the joined rdd to be a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(metadata_line, review_line):\n",
    "    \"\"\"merges the two dictionaries together\n",
    "    \n",
    "    Args:\n",
    "        metadata_line (dict): product metadata record\n",
    "        review_line (dict): review record line\n",
    "    \n",
    "    Returns:\n",
    "        new_dict (dict): merged version of product review and metadata\n",
    "    \"\"\"\n",
    "    new_dict = review_line\n",
    "    new_dict.update(metadata_line)\n",
    "    return new_dict\n",
    "\n",
    "nice_joined = joined.mapValues(lambda (meta, review): merge_dictionaries(meta, review)).values()\n",
    "row0, row1 = nice_joined.take(2)\n",
    "\n",
    "print \"row 0:\\n\\n{0}\\n\\nrow 1:\\n\\n{1}\\n\".format(row0, row1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have joined two data sources, we can start doing some ad-hoc analysis of the data! Now the task is **to get the average product review length for each category**. The categories are encoded as a list of categories, so we first need to 'flatten them out'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_joined.cache()\n",
    "nice_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_categories = nice_joined.map(lambda x: x[CATEGORIES_FIELD])\n",
    "flat_categories = nice_joined.flatMap(lambda x: x[CATEGORIES_FIELD])\n",
    "\n",
    "print \"original_categories.take(5):\\n\"\n",
    "print '\\n'.join([str(x) for x in original_categories.take(5)]) + '\\n'\n",
    "\n",
    "print \"flat_categories.take(5):\\n\"\n",
    "print '\\n'.join([str(x) for x in flat_categories.take(5)]) + '\\n'\n",
    "\n",
    "# How many distinct categories are there??? \n",
    "num_categories = flat_categories.distinct().count()\n",
    "print \"There are {0} distinct categories.\".format(num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_joined.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to get the average review length across all categories, we will use a new function: `groupByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FIELD = 'reviewText'\n",
    "\n",
    "category_review = nice_joined.flatMap(lambda x: [(y, len(x[TEXT_FIELD])) for y in x[CATEGORIES_FIELD]])\n",
    "print \"After the flatMap: \" + str(category_review.first())\n",
    "print \"After the groupByKey: \" + str(category_review.groupByKey().mapValues(list).first())\n",
    "print\n",
    "\n",
    "grouped_category_review = category_review.groupByKey().mapValues(lambda x: (sum(x)/float(len(x))))\n",
    "print \"grouped_category_review.first(): \" + str(grouped_category_review.first()) + '\\n'\n",
    "\n",
    "### Now we can sort the categories by average product review length\n",
    "print \"The top 10 categories are: \" + str(sorted(grouped_category_review.collect(), key=lambda (cat, len): len, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Do the same thing, but this time you are not allowed to use groupByKey()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Data skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data skewness is one of the common problems with Big Data.Having skewed data can affect both the computation cost and the stability of the cluster.\n",
    " \n",
    " To understand skew, first lets created a normal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1000\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(0,num):\n",
    "    for j in range(0,i):\n",
    "        data.append((i,j))\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we introduce a skewed key to this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_num = 1000000\n",
    "\n",
    "skew_data = data\n",
    "\n",
    "for i in range(0, big_num):\n",
    "    skew_data.append((big_num, i))\n",
    "\n",
    "len(skew_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load this data to a spark RDD and run a shuffle (`groupByKey()`) to see how the skew affects the computation resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = skew_data\n",
    "\n",
    "rdd = sc.parallelize(dataset)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_rdd = rdd.groupByKey().cache()\n",
    "mapped_rdd = grouped_rdd.map(lambda (k, v): (k, [(i + 10) for i in v]))\n",
    "mapped_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Integrating Spark with popular Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "model = pickle.load(open('Data/classifiers/classifier.pkl', 'r'))\n",
    "model\n",
    "bla = fashion.map(lambda x: eval(x)['reviewText']).first()\n",
    "model_b = sc.broadcast(model)\n",
    "fashion.map(lambda x: eval(x)['reviewText']).map(lambda x: (x, model_b.value.predict([x])[0])).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Part 2: Spark DataFrame API and Spark SQL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is the latter part of the tutorial. The main focus will be on Spark DataFrames and Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_filepaths = 'Data/Reviews/*'\n",
    "textRDD = sc.textFile(review_filepaths)\n",
    "\n",
    "print 'number of reviews : {0}'.format(textRDD.count())\n",
    "\n",
    "print 'sample row : \\n{0}'.format(textRDD.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into a DataFrame\n",
    "\n",
    "A DataFrame requires schema. There are two main functions that can be used to assign schema into an RDD. \n",
    "+ Inferring Schema : This functions infers the schema of the RDD by observing it\n",
    "+ Applying Schema  : This function applies a manually defined schema an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need SQL context do \n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# # Instantiate SQL Context\n",
    "sqlContext = SQLContext(sc)\n",
    "# sqlContext\n",
    "\n",
    "# print sqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredDF = sqlContext.read.json(review_filepaths)\n",
    "inferredDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Specifying the Schema\n",
    "\n",
    "The Documentation about different data types can be found at [Spark SQL DataTypes section](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-types \"Spark SQL DataTypes Documentation\") \n",
    "+ Defining the schema can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the modules\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define Schema\n",
    "REVIEWS_SCHEMA_DEF = StructType([\n",
    "        StructField('reviewerID', StringType(), True),\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('reviewerName', StringType(), True),\n",
    "        StructField('helpful', ArrayType(\n",
    "                IntegerType(), True), \n",
    "            True),\n",
    "#         add review text here\n",
    "        StructField('reviewTime', StringType(), True),\n",
    "        StructField('overall', DoubleType(), True),\n",
    "        StructField('summary', StringType(), True),\n",
    "        StructField('unixReviewTime', LongType(), True)\n",
    "    ])\n",
    "\n",
    "print REVIEWS_SCHEMA_DEF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*QUESTION*: What do you think will happen if *QUESTION*: What do you think will happen if we remove some fields from this schema?\n",
    "\n",
    "1. The schema fails\n",
    "2. The schema works fine\n",
    "\n",
    "ANSWER???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a handcrafted schema with to create a DataFrame\n",
    "appliedDF = sqlContext.read.json(review_filepaths)\n",
    "appliedDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. DataFrame operations</h1>\n",
    "\n",
    "Spark DataFrame API allow you to do multiple operations on the Data. The primary advantage of using the DataFrame API is that you can do data transoformations with the high level API without having to use Python. Using the high level API has its advantages which will be explained later in the tutorial.\n",
    "\n",
    "DataFrame API have functionality similar to that of Core RDD API. For example: \n",
    "+ map                     : foreach, Select\n",
    "+ mapPartition            : foreachPartition\n",
    "+ filter                  : filter\n",
    "+ groupByKey, reduceByKey : groupBy \n",
    "\n",
    "<h2>6.1. Selecting Columns</h2>\n",
    "\n",
    "You can use SELECT statement to select columns from your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnDF = appliedDF.select(appliedDF.asin,\n",
    "                            appliedDF.overall,\n",
    "                            appliedDF.reviewText,\n",
    "#                             add a meaningful helpful column\n",
    "                            appliedDF.reviewerID,\n",
    "                            appliedDF.unixReviewTime).\\\n",
    "                    withColumnRenamed('(helpful[0] / helpful[1])','helpful')\n",
    "columnDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Similar to Pandas, DataFrames come equipped with functions to address missing data.\n",
    "+ dropna function: can be used to remove observations with missing values\n",
    "+ fillna function: can be used to fill missing values with a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get null observations out\n",
    "\n",
    "# need to drop rows that dont have overall score\n",
    "# need 0.0 as the default value in helpful column \n",
    "densedDF= ???\n",
    "densedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering rows\n",
    "\n",
    "Filtering lets you select rows based on arguments. The implementation pattern is similar to filtering RDDs, But simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF= ??? # densedDF.overall>=3\n",
    "filteredDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by overall scores\n",
    "\n",
    "Grouping is equivalent to the groupByKey in the core RDD API. You can transform the grouped values using a summary action such as:\n",
    "+ count\n",
    "+ sum\n",
    "+ average\n",
    "+ max and so on ...\n",
    "\n",
    "\n",
    "Lets gropu the numbre of reviews by the overall score in the reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = ???\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining DataFrames together\n",
    "\n",
    "You can join two DataFrames together by using a common key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_filepaths = 'Data/Products/*'\n",
    "productRDD = sc.textFile(product_filepaths)\n",
    "productRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset2 : Amazon Product information\n",
    "# First, define Schema for second Dataset\n",
    "PRODUCTS_SCHEMA_DEF = StructType([\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('title', StringType(), True),\n",
    "        StructField('price', DoubleType(), True),\n",
    "        StructField('categories', ArrayType(ArrayType(\n",
    "            StringType(), True),True),True)\n",
    "    ])\n",
    "\n",
    "# Load the dataset\n",
    "productDF = sqlContext.read.json(product_filepaths,PRODUCTS_SCHEMA_DEF)\n",
    "productDF.show()\n",
    "# productDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us join the two datasets together based on `asin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichedReviews = (filteredDF.join(productDF, \n",
    "                                   ???).\n",
    "                              dropna(subset=\"title\"))\n",
    "enrichedReviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you join two RDDs, you have to restructure the data into (k,V) pairs where the key is the join key. This may involve two additional map transformations. This is not necessary in DataFrames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrichedReviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your DataFrame \n",
    "\n",
    "Now that we have done some operations on the data, we can save the file for later use. Standard data formats are a great way to opening up valuable data to your entire organization. Spark DataFrames can be saved in many different formats including and not limited to JSON, parquet, Hive and etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    columnDF.write.parquet('Data/Outputs/reviews_filtered.parquet')\n",
    "    print \"Saved as parquet successfully\"\n",
    "except:\n",
    "    print \"ERROR !!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark SQL\n",
    "\n",
    "Spark DataFrames also allow you to use Spark SQL to query from Petabytes of data. Spark comes with a SQL like query language which can be used to query from Distributed DataFrames. A key advantage of using Spark SQL is that the [Catelyst query optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Catelyst\") under the hood transforms your SQL query to run it most efficiently. \n",
    "\n",
    "## Example Queries\n",
    "\n",
    "Spark SQL can leverage the same functionality as the DataFrame API provides. In fact, it provides more functionality via SQL capabilities and HQL capabilities that are available to Spark SQL environment. \n",
    "\n",
    "For the sake of time constrains, I will explain different functions available in Spark SQL environment by using examples that use multiple functions. This will benefit by:\n",
    "+ Covering many functions that are possible via spark SQL\n",
    "+ Giving an understanding about how to pipe multiple functions together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reviews parquet file\n",
    "reviewsDF = sqlContext.read.parquet('Data/Outputs/reviews_filtered.parquet')\n",
    "\n",
    "# Register the DataFrames to be used in sql\n",
    "reviewsDF.registerTempTable(\"reviews\")\n",
    "productDF.registerTempTable(\"products\")\n",
    "\n",
    "print 'There are {0} reviews about {1} products'.format(reviewsDF.count(),productDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"SELECT reviews.asin, overall, reviewText, price\n",
    "            FROM reviews JOIN products ON  reviews.asin=products.asin\n",
    "            WHERE price > 50.00\n",
    "\"\"\"\n",
    "\n",
    "result = sqlContext.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional: User Defined Functions</h1>\n",
    "\n",
    "Spark SQL also provides the functionality similar to User Defined Functions (UDF) offering in Hive. Spark uses registerFunction() function to register python functions in SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def transform_review(review):\n",
    "    x1 = re.sub('[^0-9a-zA-Z\\s]+','',review)\n",
    "    return x1.lower()\n",
    "\n",
    "result.registerTempTable(\"result\")\n",
    "sqlContext.registerFunction(\"to_lowercase\", lambda x:transform_review(x), returnType=StringType())\n",
    "\n",
    "sql_query_transform = \"\"\"SELECT asin, reviewText, to_lowercase(reviewText) as cleaned\n",
    "            FROM result\n",
    "\"\"\"\n",
    "\n",
    "result_transform = sqlContext.sql(sql_query_transform)\n",
    "result_transform.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional : Mix and Match!!</h1>\n",
    "\n",
    "You can also mix DataFrames, RDDs and SparkSQL to make it work for you. \n",
    "\n",
    "<h2>Scenario</h2>\n",
    "\n",
    "We want to investigate the average rating of reviews in terms of the categories they belong to. In order to do this, we:\n",
    "+ query the needed data using DataFrames API\n",
    "+ classify the reviews into different categories using core RDD API\n",
    "+ query the avearage rating for each category using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import cPickle\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model = cPickle.load(open('Data/classifiers/classifier.pkl', 'r'))\n",
    "classifier_b = sc.broadcast(model)\n",
    "\n",
    "classifiedRDD = result_transform.filter(\"cleaned <> ''\")\\\n",
    "                                .rdd.map(lambda row: \n",
    "                                     (row.asin,row.reviewText,str(classifier_b.value.predict([row.reviewText])[0]))\n",
    "                                    )\n",
    "\n",
    "CLASSIFIED_SCHEMA = StructType([\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('review', StringType(), True),\n",
    "        StructField('category', StringType(), True)\n",
    "    ])\n",
    "\n",
    "classifiedDF = sqlContext.createDataFrame(classifiedRDD,CLASSIFIED_SCHEMA)\n",
    "\n",
    "classifiedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiedDF.registerTempTable('enrichedReviews')\n",
    "\n",
    "sql_query_test = \"\"\"SELECT category, avg(overall) as avgRating\n",
    "            FROM reviews \n",
    "            JOIN products ON reviews.asin=products.asin \n",
    "            JOIN enrichedReviews ON products.asin=enrichedReviews.asin\n",
    "            WHERE price > 50.0\n",
    "            GROUP BY enrichedReviews.category\n",
    "\"\"\"\n",
    "\n",
    "resultTest = sqlContext.sql(sql_query_test)\n",
    "resultTest.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
